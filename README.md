# ğŸ¦ Bank Data Pipeline: Scalable ETL with PySpark & Airflow

## ğŸ“Œ DescripciÃ³n del Proyecto
Este proyecto es una soluciÃ³n **End-to-End (E2E)** de IngenierÃ­a de Datos diseÃ±ada para procesar y analizar mÃ¡s de **1 millÃ³n de transacciones bancarias**. El objetivo es transformar datos financieros crudos en activos estratÃ©gicos, garantizando la **idempotencia**, la integridad de la informaciÃ³n y el cumplimiento de estÃ¡ndares de calidad exigidos en el sector bancario.

Se implementÃ³ una arquitectura distribuida que separa la orquestaciÃ³n del procesamiento masivo, simulando un entorno de producciÃ³n real utilizando contenedores.

---

## ğŸ› ï¸ Stack TecnolÃ³gico
* **OrquestaciÃ³n:** **Apache Airflow** (GestiÃ³n y monitoreo de flujos de trabajo).
* **Procesamiento Big Data:** **Apache Spark (PySpark)** (Motor de procesamiento distribuido para grandes volÃºmenes).
* **ContenerizaciÃ³n:** **Docker & Docker Compose** (Aislamiento de servicios y entorno replicable).
* **Almacenamiento:** **Parquet (Snappy compression)** (OptimizaciÃ³n de almacenamiento y velocidad de consulta).
* **Lenguaje y LibrerÃ­as:** **Python** (Pandas, NumPy, PySpark SQL).
* **Base de Datos de Metadatos:** **PostgreSQL** (Persistencia del historial de ejecuciÃ³n de Airflow).

---

## ğŸš€ Arquitectura del Pipeline
El flujo de datos se divide en cuatro etapas crÃ­ticas gestionadas por un DAG (Directed Acyclic Graph) en Airflow:

1. **Ingesta Automatizada:** Carga masiva de archivos CSV desde el Data Lake local hacia el entorno distribuido.
2. **ValidaciÃ³n & Data Quality:**
    * Filtrado de anomalÃ­as como montos negativos o transacciones incoherentes.
    * Manejo de valores nulos y estandarizaciÃ³n de esquemas tÃ©cnicos.
3. **Procesamiento Distribuido:**
    * CÃ¡lculo de balances promedio por ubicaciÃ³n geogrÃ¡fica mediante Spark SQL.
    * SegmentaciÃ³n de clientes basada en comportamiento transaccional masivo.
4. **Carga Optimizada:** ExportaciÃ³n de resultados a archivos Parquet particionados, mejorando el rendimiento de futuras consultas analÃ­ticas.

---

## ğŸ“‚ Estructura del Repositorio

Bank-Data-Pipeline/
â”œâ”€â”€ dags/                   # DefiniciÃ³n de flujos y tareas en Airflow
â”œâ”€â”€ src/                    # Scripts de procesamiento PySpark y utilitarios
â”‚   â”œâ”€â”€ etl_process.py      # LÃ³gica principal de transformaciÃ³n
â”‚   â””â”€â”€ utils.py            # Funciones de validaciÃ³n de calidad y limpieza
â”œâ”€â”€ data/                   # Data Lake local (Raw y Processed)
â”œâ”€â”€ docker-compose.yml      # ConfiguraciÃ³n de servicios (Airflow, Spark, Postgres)
â””â”€â”€ README.md               # DocumentaciÃ³n del proyecto