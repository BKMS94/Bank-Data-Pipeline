# üè¶ Bank Transactions ETL Pipeline (PySpark + Airflow)

Este proyecto implementa un pipeline de datos robusto para la ingesta, transformaci√≥n y disponibilidad de transacciones financieras. Est√° dise√±ado bajo principios de **idempotencia**, **resiliencia** y **calidad de datos**, simulando un entorno de banca comercial regulado.



## üèóÔ∏è Arquitectura del Proyecto
El proyecto utiliza una estructura modular para separar la orquestaci√≥n de la l√≥gica de negocio, facilitando el mantenimiento y la escalabilidad:

* **`dags/`**: Contiene el orquestador `bank_pipeline_dag.py` que gestiona la ejecuci√≥n y reintentos del flujo.
* **`src/`**: N√∫cleo t√©cnico del proyecto con `etl_process.py` para la l√≥gica ETL y `utils.py` para configuraciones transversales.
* **`data/`**: Zona de aterrizaje (Landing Zone) para los archivos fuente `bank_transactions.csv`.
* **`drivers/`**: Almacena el conector JDBC necesario para la persistencia en base de datos.

## üõ†Ô∏è Stack Tecnol√≥gico
* **Orquestaci√≥n**: Apache Airflow 2.7.1.
* **Procesamiento**: PySpark 3.5.0 (Computaci√≥n distribuida).
* **Contenerizaci√≥n**: Docker & Docker Compose.
* **Base de Datos**: PostgreSQL 13 (Data Mart anal√≠tico).
* **Lenguaje**: Python 3.9.

## üöÄ Caracter√≠sticas Principales (Valor T√©cnico)
1.  **Idempotencia**: Implementaci√≥n de carga en modo `overwrite` y eliminaci√≥n de duplicados mediante `TransactionId` para garantizar que ejecuciones repetidas no corrompan el destino.
2.  **Calidad de Datos (Data Quality)**: Filtros de validaci√≥n para montos negativos y tratamiento de valores nulos antes de la persistencia.
3.  **Trazabilidad y Linaje**: Inserci√≥n de metadatos de auditor√≠a
